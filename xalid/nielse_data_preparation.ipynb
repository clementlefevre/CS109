{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import date\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import PandasHelper as pdh\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'\n",
    "MAIN_FILE = DATA_PATH+'nielsen.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_name</th>\n",
       "      <th>first_timeframe</th>\n",
       "      <th>dwell_time_s</th>\n",
       "      <th>device_id</th>\n",
       "      <th>visitor</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BF Karlsruhe Kaiserstr (1122)</td>\n",
       "      <td>2014-12-31 23:00:30</td>\n",
       "      <td>15</td>\n",
       "      <td>bd5d8c2890622782d681c82f4dd84db4</td>\n",
       "      <td>True</td>\n",
       "      <td>2014-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BF Karlsruhe Kaiserstr (1122)</td>\n",
       "      <td>2014-12-31 23:00:40</td>\n",
       "      <td>1080</td>\n",
       "      <td>428fa91d6d741e1466b4bcd917dff4c2</td>\n",
       "      <td>True</td>\n",
       "      <td>2014-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       site_name     first_timeframe  dwell_time_s                         device_id visitor        date\n",
       "0  BF Karlsruhe Kaiserstr (1122) 2014-12-31 23:00:30            15  bd5d8c2890622782d681c82f4dd84db4    True  2014-12-31\n",
       "1  BF Karlsruhe Kaiserstr (1122) 2014-12-31 23:00:40          1080  428fa91d6d741e1466b4bcd917dff4c2    True  2014-12-31"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(MAIN_FILE, parse_dates=['first_timeframe'],nrows=1000)\n",
    "df['date']=df['first_timeframe'].dt.date\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Create indexes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk_and_retrieve_indexes():\n",
    "    chunks = pd.read_csv(MAIN_FILE,chunksize=1000000)\n",
    "    \n",
    "    for i,chunk in enumerate(chunks):\n",
    "        print chunk.shape\n",
    "        devices_ix = pd.DataFrame(chunk.device_id.unique())\n",
    "        sites_ix = pd.DataFrame(chunk.site_name.unique())\n",
    "        devices_ix.to_csv(DATA_PATH+\"indexes/devices/devices_ix_\"+str(i)+\".csv\")\n",
    "        sites_ix.to_csv(DATA_PATH+\"indexes/sites/sites_ix_.csv\"+str(i)+\".csv\")\n",
    "\n",
    "def concatenate_df(df,path):\n",
    "    df1 = pd.read_csv(path,index_col=0)\n",
    "   \n",
    "    return pd.concat([df,df1]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def concatenate_index_files():\n",
    "    index_devices_files = pdh.get_files(DATA_PATH+\"indexes/devices/\")\n",
    "    index_sites_files = pdh.get_files(DATA_PATH+\"indexes/sites/\")\n",
    "    \n",
    "    df_devices_ix = pd.DataFrame()\n",
    "    df_sites_ix = pd.DataFrame()\n",
    "    \n",
    "    for index_device_file in index_devices_files:\n",
    "        print index_device_file\n",
    "        df_devices_ix = concatenate_df(df_devices_ix,DATA_PATH+\"indexes/devices/\"+index_device_file)\n",
    "    df_devices_ix.columns=['id','device_mac']\n",
    "    df_devices_ix.to_csv(DATA_PATH+\"indexes/devices_ix.csv\")\n",
    "    \n",
    "    for index_sites_file in index_sites_files:\n",
    "        df_sites_ix = concatenate_df(df_sites_ix,DATA_PATH+\"indexes/sites/\"+index_sites_file)\n",
    "    df_sites_ix.columns=['id','site_name']\n",
    "    df_sites_ix.to_csv(DATA_PATH+\"indexes/sites_ix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-replace devices and sites name with ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_agg_with_device_and_sites_index():\n",
    "    chunks=pd.read_csv(DATA_PATH+\"nielsen.csv\",chunksize=1000000)\n",
    "    df_devices_idx = pd.read_csv(DATA_PATH+\"indexes/devices_ix.csv\")\n",
    "    df_sites_index = pd.read_csv(DATA_PATH+\"indexes/sites_ix.csv\")\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for i,chunk in enumerate(chunks):\n",
    "        print chunk.shape, i\n",
    "        chunk = pd.merge(chunk, df_devices_idx, left_on='device_id', right_on='device_mac',suffixes=('_agg', '_devicesidx'))\n",
    "        chunk = chunk[['id','site_name','first_timeframe']]\n",
    "        chunk = pd.merge(chunk, df_sites_index, on='site_name', suffixes=('_devices','_sites'))\n",
    "        chunk=chunk[['id_devices', 'id_sites']]\n",
    "        chunk=chunk.astype('int32')\n",
    "        df = pd.concat([df,chunk], axis=0, ignore_index=True)\n",
    "    print \"merge over,starting saving to disk...\"\n",
    "    df.to_csv(DATA_PATH+'/nielsen_indexes.csv')\n",
    "    print \"saved to disk OK.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Aggregate per devices and site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggregate(chunk):\n",
    "    \n",
    "    chunk['count']=1\n",
    "    groupy = chunk.groupby(['id_devices','id_sites']).count()\n",
    "    groupy = groupy.reset_index()\n",
    "    \n",
    "    groupy =groupy.groupby('id_devices')['id_sites'].apply(lambda x: x.tolist())\n",
    "    return pd.DataFrame(groupy)\n",
    "\n",
    "def aggregate_per_devices_and_sites():\n",
    "    chunks = pd.read_csv(DATA_PATH+'/nielsen_indexes.csv',chunksize=40000000,index_col=0)\n",
    "    df =pd.DataFrame()\n",
    "    \n",
    "    for i,chunk in enumerate(chunks):\n",
    "        df = pd.concat([df,aggregate(chunk)], axis=1, ignore_index=False)\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        print df.shape,i\n",
    "    df.to_csv(DATA_PATH+'/nielsen_indexes_pivot.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4- Clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combine_sites_id(df):\n",
    "    cols = df.columns.tolist()\n",
    "    df = df.replace('0', 0)\n",
    "    df =df.replace(0,\"\")\n",
    "    df = df.fillna(\"\")\n",
    "    df['sites_id']= df[cols].astype(str).sum(axis=1)\n",
    "    df['sites_id_array']= df['sites_id'].apply(pdh.string_to_np_array)\n",
    "    df['sites_count']= df['sites_id_array'].apply(lambda x: x.size)\n",
    "    df = df[['sites_id_array','sites_count']]\n",
    "    df.reset_index()\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def clean_data():\n",
    "    chunks = pd.read_csv(DATA_PATH+'nielsen_indexes_pivot.csv',chunksize=1000000,index_col=0)\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for i,chunk in enumerate(chunks):\n",
    "        df = pd.concat([df,combine_sites_id(chunk)],axis=0)\n",
    "        print df.shape,i\n",
    "    print \"Combining done, saving to file...\"\n",
    "    df.to_csv(DATA_PATH+\"nielsen_indexes_sites_per_devices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5- Filter data on devices that appeared on more than one site\n",
    "once a list of devices/sites/number of sites detected has been created, we can easily filter on devices with multiple site appearances to reduce the dataset of 67%, to a dataset of around 6 millions devices :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_mask_on_devices_that_appears_on_multiple_sites():\n",
    "    df = pd.read_csv(DATA_PATH+\"/nielsen_indexes_sites_per_devices.csv\", index_col=0)\n",
    "    mask = pd.DataFrame(df[df.sites_count>1].index.values)\n",
    "    mask.columns=['device_id']\n",
    "    \n",
    "    df_devices_indexes = pd.read_csv(DATA_PATH+\"indexes/devices_ix.csv\")\n",
    "    df = pd.merge(mask,df_devices_indexes,left_on=\"device_id\",right_on=\"id\")\n",
    "    df=df[['device_id','device_mac']]\n",
    "    df.to_csv(DATA_PATH+\"indexes/devices_ix_multiple_sites.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Merge devices with multiple sites with raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_data(chunk,mask,sites_id):\n",
    "    chunk = chunk.reset_index()\n",
    "    chunk['date']=chunk['first_timeframe'].dt.date\n",
    "\n",
    "    df1 = pd.merge(chunk,mask,left_on='device_id',right_on='device_mac')[['device_id_y','site_name','date']]\n",
    "    df1 = pd.merge(df1,sites_id,on='site_name')[['device_id_y','id','date']]\n",
    "    df1.columns = ['device_id','site_id','date']\n",
    "    return df1\n",
    "\n",
    "def reduce_data_on_mask():\n",
    "    devices_id_chunks = pd.read_csv(DATA_PATH+\"indexes/devices_ix_multiple_sites.csv\", chunksize=500000,index_col=0)\n",
    "    sites_id = pd.read_csv(DATA_PATH+\"indexes/sites_ix.csv\",index_col=0)\n",
    "    sites_id = sites_id.reset_index()  \n",
    "    \n",
    "    for i,devices_id_chunk in enumerate(devices_id_chunks):\n",
    "        data_chunks = pd.read_csv(DATA_PATH+'nielsen.csv',parse_dates=['first_timeframe'],chunksize=1000000,index_col=0)\n",
    "        print \"Merging \"+str(i)+\" Starts.------------------------------\"\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        for j,data_chunk in enumerate(data_chunks):\n",
    "            df = pd.concat([df,merge_data(data_chunk,devices_id_chunk,sites_id)],axis=0)\n",
    "            print df.shape,j\n",
    "        print \"Finished merging, saving to file...\"\n",
    "        df.to_csv(DATA_PATH+\"nielsen_data_multiple_sites_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-Group filtered raw data per device-date-site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def final_count(file_name):\n",
    "    df = pd.read_csv(DATA_PATH+\"data_multiples_sites_only_with_indexes/\"+file_name,index_col=0)\n",
    "    result = df.groupby(['device_id','date'])['site_id'].apply(lambda x: (np.unique(x.values).size))\n",
    "    final = pd.DataFrame(result)\n",
    "    final.columns=[['count_sites']]\n",
    "    file_ix = re.findall(r'\\d+',file_name)[0]\n",
    "    final.to_csv(DATA_PATH+\"groupby_device_date_site/groupby_device_date_site\"+str(file_ix)+\".csv\")\n",
    "    \n",
    "def groupby_device_date_site():\n",
    "    files = pdh.get_files(DATA_PATH+\"data_multiples_sites_only_with_indexes/\")\n",
    "    \n",
    "    for filo in files:\n",
    "        print \"grouping :\"+ filo\n",
    "        final_count(filo)\n",
    "    \n",
    "    groupby_files = pdh.get_files(DATA_PATH+\"groupby_device_date_site/\")\n",
    "    df = pd.DataFrame()\n",
    "    for groupbyfilo in groupby_files:\n",
    "        df1 = pd.read_csv(DATA_PATH+\"groupby_device_date_site/\"+groupbyfilo,index_col=0)\n",
    "        df = pd.concat([df,df1],axis=0)\n",
    "    df.to_csv(DATA_PATH+\"groupby_device_date_site_final.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8- Some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dataset_stats():\n",
    "    chunks = pd.read_csv(DATA_PATH+\"nielsen.csv\",parse_dates=['first_timeframe'],chunksize=1000000)\n",
    "    stats =  {}\n",
    "    min_date = date.today()\n",
    "    max_date = date(2000,1,1)\n",
    "    rows =0\n",
    "    for c in chunks:\n",
    "        rows+=c.shape[0]\n",
    "        c.first_timeframe = c.first_timeframe.dt.date\n",
    "        min_c,max_c = c.first_timeframe.min(), c.first_timeframe.max()\n",
    "        if min_c < min_date:\n",
    "            min_date = min_c\n",
    "        if max_c > max_date:\n",
    "            max_date = max_c\n",
    "        print \"{:,}\".format(rows),min_date,max_date\n",
    "    stats = {'rows':rows,'min_date':min_date,'max_date':max_date}            \n",
    "    return stats\n",
    "\n",
    "\n",
    "def get_number_of_devices():\n",
    "    df = pd.read_csv(DATA_PATH+\"indexes/devices_ix.csv\",index_col=0)\n",
    "    return df.shape[0]\n",
    "\n",
    "def get_number_of_sites():\n",
    "    df = pd.read_csv(DATA_PATH+\"indexes/sites_ix.csv\",index_col=0)\n",
    "    return df.shape[0]\n",
    "\n",
    "def get_numbers_records_multi_sites_devices():\n",
    "    rows = 0\n",
    "    files = pdh.get_files(DATA_PATH+\"data_multiples_sites_only_with_indexes/\")\n",
    "    for filo in files:\n",
    "        df1 = pd.read_csv(DATA_PATH+\"data_multiples_sites_only_with_indexes/\"+filo,index_col=0) \n",
    "        rows+=df1.shape[0]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <pre>\n",
    "<center>\n",
    "\n",
    "| Dataset total records |  167,944,229 |\n",
    "| :- | :-:\n",
    "|   Number of devices  | 21,010,205|\n",
    "|   Number of sites  | 53|\n",
    "|   Number of devices on more than 2 sites (multi-sites devices) | 1,994,526|\n",
    "|   Records for multi-sites devices  | 47,965,794|\n",
    "|   Date range of records  | 2014-12-31 to 2015-12-31|\n",
    "\n",
    "</center>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_visits_repartition():\n",
    "    df = pd.read_csv(DATA_PATH+\"groupby_device_date_site_final.csv\",index_col=0)\n",
    "    df.columns=[['date','count_sites']]\n",
    "    groupy = df.reset_index()\n",
    "    groupy['date']= pd.to_datetime(groupy.date)\n",
    "    groupy['weekday']= groupy['date'].dt.dayofweek\n",
    "    groupy['day_count']= (groupy.date-date(2014,12,31)) / np.timedelta64(1, 'D')\n",
    "    \n",
    "    return groupy\n",
    "\n",
    "def scatter_visits(groupy):\n",
    "    groupy_more_than_1_site = groupy[groupy.count_sites>1]\n",
    "    plt.scatter(groupy_more_than_1_site.day_count,groupy_more_than_1_site.count_sites,alpha=0.7, c='r')\n",
    "    plt.xlabel(\"day from 1st Jan 2015\", fontsize=12)\n",
    "    plt.ylabel(\"Number of sites visited per day\",fontsize=12)\n",
    "    plt.title('Visited sites per device')\n",
    "    visited_sites_per_device_per_year = pd.DataFrame(groupy.groupby('device_id')['count_sites'].mean())\n",
    "    visited_sites_per_device_per_year = visited_sites_per_device_per_year.sort_values('count_sites',ascending=False)\n",
    "    visited_sites_per_device_per_year= visited_sites_per_device_per_year.reset_index()\n",
    "    groupy_visits = visited_sites_per_device_per_year.groupby(['count_sites']).count()\n",
    "    groupy_visits= groupy_visits.sort_values(by=\"device_id\",ascending=False)\n",
    "    print groupy_visits.device_id.sum(axis=0)\n",
    "    groupy_visits['cumsumo']=groupy_visits.device_id/groupy_visits.device_id.sum(axis=0)*100\n",
    "    return groupy_visits[groupy_visits.device_id>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_visits(groupy):\n",
    "    devices_per_day_weekday = groupy[groupy.weekday<6]\n",
    "    devices_per_day_weekend = groupy[groupy.weekday==6]\n",
    "    devices_per_day_weekday = devices_per_day_weekday.groupby('date')['count_sites'].count()\n",
    "    devices_per_day_weekend = devices_per_day_weekend.groupby('date')['count_sites'].count()\n",
    "    devices_per_day_weekday = devices_per_day_weekday.sort_index(ascending=True)\n",
    "    devices_per_day_weekend = devices_per_day_weekend.sort_index(ascending=True)\n",
    "    devices_per_day_weekday.plot(alpha=0.7,c='g',title=\"Number of multi-sites devices recorded per day\")\n",
    "    devices_per_day_weekend.plot(alpha=0.7,c='r',legend='Sunday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_device_mac(device_id_list):\n",
    "    df = pd.read_csv(DATA_PATH+\"/indexes/devices_ix_multiple_sites.csv\")\n",
    "    return df[df.device_id.isin(device_id_list)]\n",
    "\n",
    "def search_device_data(mac_list):  \n",
    "    chunks = pd.read_csv(DATA_PATH+\"/nielsen.csv\",chunksize = 1000000)\n",
    "    df = pd.DataFrame()\n",
    "    for i,c in enumerate(chunks):\n",
    "        df1 = c[c.device_id.isin(mac_list)]\n",
    "        print df1.shape,i\n",
    "        df = pd.concat([df,df1],axis=0)\n",
    "    df.to_csv(DATA_PATH+\"search_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = display_visits_repartition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>date</th>\n",
       "      <th>count_sites</th>\n",
       "      <th>weekday</th>\n",
       "      <th>day_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2224071</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2224071</td>\n",
       "      <td>2015-01-13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2224071</td>\n",
       "      <td>2015-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2224071</td>\n",
       "      <td>2015-01-26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2224071</td>\n",
       "      <td>2015-01-29</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   device_id       date  count_sites  weekday  day_count\n",
       "0    2224071 2015-01-05            1        0        5.0\n",
       "1    2224071 2015-01-13            1        1       13.0\n",
       "2    2224071 2015-01-20            1        1       20.0\n",
       "3    2224071 2015-01-26            1        0       26.0\n",
       "4    2224071 2015-01-29            1        3       29.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_sites</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>device_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749151</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749147</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749136</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749097</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count_sites\n",
       "device_id             \n",
       "24                 1.0\n",
       "4749151            1.0\n",
       "4749147            1.0\n",
       "4749136            1.0\n",
       "4749097            1.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visits =counts.groupby('device_id')['count_sites'].mean()\n",
    "visits = pd.DataFrame(visits)\n",
    "visits= visits.sort_values(by='count_sites',ascending=True)\n",
    "devices = visits[visits.count_sites==1].index.tolist()[:10]\n",
    "devices+= visits[visits.count_sites>4].index.tolist()[:10]\n",
    "visits.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device_mac_list = search_device_mac(devices).device_mac.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['76337b2689839e68b22235bdbb00ae5f',\n",
       " '71085a86d20b64c43ec3b9781ef70cfc',\n",
       " 'fd5ab440ac21cc7726883d844b119861',\n",
       " '96b40b45252bdcf12ffc90a1c5995f96',\n",
       " '1d378b6df55bcc9dea58b6728e7e8285',\n",
       " 'd412d012c3961a85258a970f1e19c82b',\n",
       " '873dc43cc36d8b66dd295625ec9c2b56',\n",
       " '200f14d66a5e433d1de35ca38b15224d',\n",
       " '86e5f3f97ae1c2f87d6dcfd066b907c1',\n",
       " 'a470147fa8e70547a380b6b3404bea37',\n",
       " 'cc3d9179915f97e328bbbe3959c66fa9',\n",
       " '711754e35c5dc013bd805b8a668a781f',\n",
       " '3bd28e9d0ebf7bb8475e3aa430b03298',\n",
       " '8afc3408ca592ee9d3261533d05706d6',\n",
       " 'd3c740597cf2d59b14de69c6a1a17c68',\n",
       " '9f45f76554d3a330e3c32d8e458e1fde',\n",
       " 'aeb2fa0bb47631e84a61a3b3471e9722']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_mac_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(device_mac_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH+\"/nielsen.csv\",nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 5) 0\n",
      "(210, 5) 1\n",
      "(269, 5) 2\n",
      "(297, 5) 3\n",
      "(264, 5) 4\n",
      "(304, 5) 5\n",
      "(287, 5) 6\n",
      "(270, 5) 7\n",
      "(279, 5) 8\n",
      "(263, 5) 9\n",
      "(269, 5) 10\n",
      "(362, 5) 11\n",
      "(253, 5) 12\n",
      "(256, 5) 13\n",
      "(255, 5) 14\n",
      "(176, 5) 15\n",
      "(221, 5) 16\n",
      "(196, 5) 17\n",
      "(318, 5) 18\n",
      "(189, 5) 19\n",
      "(173, 5) 20\n",
      "(275, 5) 21\n",
      "(798, 5) 22\n",
      "(367, 5) 23\n",
      "(334, 5) 24\n",
      "(310, 5) 25\n",
      "(260, 5) 26\n",
      "(221, 5) 27\n",
      "(220, 5) 28\n",
      "(189, 5) 29\n",
      "(195, 5) 30\n",
      "(144, 5) 31\n",
      "(381, 5) 32\n",
      "(305, 5) 33\n",
      "(231, 5) 34\n",
      "(206, 5) 35\n",
      "(219, 5) 36\n",
      "(175, 5) 37\n",
      "(164, 5) 38\n",
      "(190, 5) 39\n",
      "(238, 5) 40\n",
      "(367, 5) 41\n",
      "(178, 5) 42\n",
      "(244, 5) 43\n",
      "(457, 5) 44\n",
      "(124, 5) 45\n",
      "(194, 5) 46\n",
      "(271, 5) 47\n",
      "(280, 5) 48\n",
      "(306, 5) 49\n",
      "(308, 5) 50\n",
      "(244, 5) 51\n",
      "(222, 5) 52\n",
      "(201, 5) 53\n",
      "(226, 5) 54\n",
      "(209, 5) 55\n",
      "(131, 5) 56\n",
      "(516, 5) 57\n",
      "(257, 5) 58\n",
      "(323, 5) 59\n",
      "(226, 5) 60\n",
      "(227, 5) 61\n",
      "(206, 5) 62\n",
      "(170, 5) 63\n",
      "(144, 5) 64\n",
      "(185, 5) 65\n",
      "(224, 5) 66\n",
      "(158, 5) 67\n",
      "(151, 5) 68\n",
      "(219, 5) 69\n",
      "(216, 5) 70\n",
      "(270, 5) 71\n",
      "(229, 5) 72\n",
      "(220, 5) 73\n",
      "(157, 5) 74\n",
      "(153, 5) 75\n",
      "(194, 5) 76\n",
      "(148, 5) 77\n",
      "(110, 5) 78\n",
      "(148, 5) 79\n",
      "(209, 5) 80\n",
      "(177, 5) 81\n",
      "(334, 5) 82\n",
      "(144, 5) 83\n",
      "(108, 5) 84\n",
      "(99, 5) 85\n",
      "(157, 5) 86\n",
      "(207, 5) 87\n",
      "(300, 5) 88\n",
      "(301, 5) 89\n",
      "(197, 5) 90\n",
      "(270, 5) 91\n",
      "(211, 5) 92\n",
      "(236, 5) 93\n",
      "(198, 5) 94\n",
      "(229, 5) 95\n",
      "(196, 5) 96\n",
      "(200, 5) 97\n",
      "(145, 5) 98\n",
      "(208, 5) 99\n",
      "(176, 5) 100\n",
      "(160, 5) 101\n",
      "(186, 5) 102\n",
      "(144, 5) 103\n",
      "(336, 5) 104\n",
      "(211, 5) 105\n",
      "(194, 5) 106\n",
      "(249, 5) 107\n",
      "(158, 5) 108\n",
      "(220, 5) 109\n",
      "(237, 5) 110\n",
      "(307, 5) 111\n",
      "(167, 5) 112\n",
      "(307, 5) 113\n",
      "(203, 5) 114\n",
      "(132, 5) 115\n",
      "(138, 5) 116\n",
      "(131, 5) 117\n",
      "(166, 5) 118\n",
      "(174, 5) 119\n",
      "(188, 5) 120\n",
      "(311, 5) 121\n",
      "(250, 5) 122\n",
      "(363, 5) 123\n",
      "(142, 5) 124\n",
      "(279, 5) 125\n",
      "(150, 5) 126\n",
      "(270, 5) 127\n",
      "(290, 5) 128\n",
      "(275, 5) 129\n",
      "(229, 5) 130\n",
      "(180, 5) 131\n",
      "(231, 5) 132\n",
      "(208, 5) 133\n",
      "(261, 5) 134\n",
      "(196, 5) 135\n",
      "(158, 5) 136\n",
      "(238, 5) 137\n",
      "(188, 5) 138\n",
      "(339, 5) 139\n",
      "(126, 5) 140\n",
      "(162, 5) 141\n",
      "(55, 5) 142\n",
      "(78, 5) 143\n",
      "(180, 5) 144\n",
      "(355, 5) 145\n",
      "(308, 5) 146\n",
      "(275, 5) 147\n",
      "(172, 5) 148\n",
      "(135, 5) 149\n",
      "(190, 5) 150\n",
      "(278, 5) 151\n",
      "(263, 5) 152\n",
      "(252, 5) 153\n",
      "(187, 5) 154\n",
      "(130, 5) 155\n",
      "(334, 5) 156\n",
      "(266, 5) 157\n",
      "(198, 5) 158\n",
      "(115, 5) 159\n",
      "(92, 5) 160\n",
      "(89, 5) 161\n",
      "(126, 5) 162\n",
      "(241, 5) 163\n",
      "(162, 5) 164\n",
      "(317, 5) 165\n",
      "(253, 5) 166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramon/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2885: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(248, 5) 167\n"
     ]
    }
   ],
   "source": [
    "search_device_data(device_mac_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH+\"search_result.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupy = df[df.device_id=='aeb2fa0bb47631e84a61a3b3471e9722']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>site_name</th>\n",
       "      <th>first_timeframe</th>\n",
       "      <th>dwell_time_s</th>\n",
       "      <th>device_id</th>\n",
       "      <th>visitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>141577</td>\n",
       "      <td>BF Berlin Brunnenstr (2123)</td>\n",
       "      <td>2015-03-31 12:46:50+00:00</td>\n",
       "      <td>220</td>\n",
       "      <td>aeb2fa0bb47631e84a61a3b3471e9722</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6226</th>\n",
       "      <td>696089</td>\n",
       "      <td>BF Berlin Friedrichstr (1113)</td>\n",
       "      <td>2015-04-16 15:54:00+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>aeb2fa0bb47631e84a61a3b3471e9722</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27763</th>\n",
       "      <td>186582</td>\n",
       "      <td>BF Kassel Obere Königsstr (1132)</td>\n",
       "      <td>2015-08-22 10:47:30+00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>aeb2fa0bb47631e84a61a3b3471e9722</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                         site_name            first_timeframe  dwell_time_s                         device_id visitor\n",
       "1661       141577       BF Berlin Brunnenstr (2123)  2015-03-31 12:46:50+00:00           220  aeb2fa0bb47631e84a61a3b3471e9722   False\n",
       "6226       696089    BF Berlin Friedrichstr (1113)   2015-04-16 15:54:00+00:00             5  aeb2fa0bb47631e84a61a3b3471e9722   False\n",
       "27763      186582  BF Kassel Obere Königsstr (1132)  2015-08-22 10:47:30+00:00            15  aeb2fa0bb47631e84a61a3b3471e9722   False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupy = groupy.groupby(['site_name'])['first_timeframe'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_name\n",
       "BF Berlin Brunnenstr (2123)         1\n",
       "BF Berlin Friedrichstr (1113)       1\n",
       "BF Kassel Obere Königsstr (1132)    1\n",
       "Name: first_timeframe, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "groupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
