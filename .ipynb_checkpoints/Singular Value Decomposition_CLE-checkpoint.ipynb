{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition and Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frank Cleary | <a href=\"http://www.frankcleary.com\">www.frankcleary.com</a> | See also: <a href=\"http://www.frankcleary.com/svdimage\">SVD Image Compression</a> | <a href=\"https://gist.github.com/frankcleary/a89da479d85c98f86e31\">Notebook Gist</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The singular value decomposition of a matrix has many applications. Here I'll focus on an introduction to singular value decomposition and an application in clustering articles by topic. In another notebook (<a href=\"http://nbviewer.ipython.org/gist/frankcleary/4d2bd178708503b556b0\">link</a>) I show how singular value decomposition can be used in image compression.\n",
    "\n",
    "Any matrix $A$ can be decomposed to three matrices $U$, $\\Sigma$, and $V$ such that $A = U \\Sigma V$, this is called singular value decomposition. The columns of $U$ and $V$ are orthonormal and $\\Sigma$ is diagonal. Most scientific computing packages have a function to compute the singular value decomposition, I won't go into the details of how to find $U$, $\\Sigma$ and $V$ here. Some sources write the decomposition as $A = U \\Sigma V^T$, so that their $V^T$ is our $V$. The usage in this notebook is consistent with how numpy's singular value decomposition function returns $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with a small matrix $A$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $A = \\begin{bmatrix} 1 & 0 \\\\ 1 & 2 \\end{bmatrix}$\n",
    "                     \n",
    "$A$ can be written as $U \\Sigma V$ where $U$, $\\Sigma$, and $V$ are, rounded to 2 decimal places:\n",
    "\n",
    "$U = \\begin{bmatrix} -0.23 & -0.97 \\\\ -0.97 & 0.23 \\end{bmatrix}$\n",
    "                     \n",
    "$S = \\begin{bmatrix} 2.29 & 0 \\\\ 0 & 0.87 \\end{bmatrix}$\n",
    "                     \n",
    "$V = \\begin{bmatrix} -0.53 & -0.85 \\\\ -0.85 & 0.53 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the singular value decomposition has interesting properties from a linear algebra standpoint, I'm going to focus here on some of its applications and skip the derivation and geometric interpretations.\n",
    "\n",
    "Let $A$ be a $m \\times n$ matrix with column vectors $\\vec{a}_1, \\vec{a}_2, ..., \\vec{a}_n$. In the singular value decomposition of $A$, $U$ will be $m \\times m$, $\\Sigma$ will be $m \\times n$ and $V$ will be $n \\times n$. We denote the column vectors of $U$ as $\\vec{u}_1, \\vec{u}_2, ..., \\vec{u}_m$ and $V$  as $\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_n$, similarly to $A$. We'll call the values along the diagonal of $\\Sigma$ as $\\sigma_1, \\sigma_2, ...$.\n",
    "\n",
    "We have that $A = U \\Sigma V$ where:\n",
    "\n",
    "$U = \\begin{bmatrix} \\\\ \\\\ \\\\ \\vec{u}_1 & \\vec{u}_2 & \\dots & \\vec{u}_m \\\\ \\\\ \\\\ \\end{bmatrix}$\n",
    "\n",
    "$\\Sigma = \\begin{bmatrix} \\sigma_1 & 0 & \\dots \\\\ 0 &  \\sigma_2 & \\dots \\\\ \\vdots & \\vdots & \\ddots \\end{bmatrix}$\n",
    "\n",
    "$V = \\begin{bmatrix} \\\\ \\\\ \\\\ \\vec{v}_1 & \\vec{v}_2 & \\dots & \\vec{v}_n \\\\ \\\\ \\\\ \\end{bmatrix}$\n",
    "\n",
    "Because $\\Sigma$ is diagonal, the columns of $A$ can be written as:\n",
    "\n",
    "$\\vec{a}_i = \\vec{u}_1 * \\sigma_1 * V_{1,i} + \n",
    "             \\vec{u}_2 * \\sigma_2 * V_{2,i} + ... = U * \\Sigma * \\vec{v}_i$\n",
    "             \n",
    "This is equivalent to creating a vector $\\vec{w}_i$, where the elements of $\\vec{w}_i$ are the elements of $\\vec{v}_i$, weighted by the $\\sigma$'s:\n",
    "\n",
    "$\\vec{w}_i = \\begin{bmatrix} \\sigma_1V_{1,i} \\\\ \\sigma_2V_{2,i} \\\\\n",
    "           \\sigma_3V_{3,i} \\\\ \\vdots \\end{bmatrix} = \\Sigma * \\vec{v}_i$\n",
    "           \n",
    "Then $\\vec{a}_i = U * \\vec{w}_i$. That is to say that every column $\\vec{a}_i$ of $A$ is expressed by a sum over all the columns of $U$, weighted by the values in the $i^{th}$ column of $V$, and the $\\sigma$'s. By convention the order of the columns in $U$ and rows in $V$ is chosen such that the values in \n",
    "$\\Sigma = \\begin{bmatrix} \\sigma_1 & 0 & \\dots \\\\ 0 &  \\sigma_2 & \\dots \\\\ \\vdots & \\vdots & \\ddots \\end{bmatrix}$ obey $\\sigma_1 > \\sigma_2 > \\sigma_3 > ...$. This means that as a whole, the first column of $U$ and the first row of $V$ contribute more to the final values of $A$ than subsequent columns. This has applications in image compression (<a href=\"http://nbviewer.ipython.org/gist/frankcleary/4d2bd178708503b556b0\">link to another notebook</a>) and reducing the dimensionality of data by selecting the most import components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief discussion of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section isn't required to understand how singular value decomposition is useful, but I've included it for completeness.\n",
    "\n",
    "If $A$ is $m \\times n$ ($m$ rows and $n$ columns), $U$ will be $m \\times m$, $\\Sigma$ will be $m \\times n$ and $V$ will be $n \\times n$. However, there are only $r = rank(A)$ non-zero values in $\\Sigma$, i.e. $\\sigma_1, ..., \\sigma_r \\neq 0$; $\\sigma_{r+1}, ..., \\sigma_n = 0$. Therefore columns of $U$ beyond the $r^{th}$ column and rows of $V$ beyond the $r^{th}$ row do not contribute to $A$ and are usually omitted, leaving $U$ an $m \\times r$ matrix, $\\Sigma$ an $r \\times r$ diagonal matrix and $V$ an $r \\times n$ matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular value decomposition can be used to classify similar objects (for example, news articles on a particular topic). Note above that similar $\\vec{a_i}$'s will have similar $\\vec{v_i}$'s.\n",
    "\n",
    "Imagine four blog posts, two about skiing and two about hockey. I've made up some data about five different words and the number of times they appear in each post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post1</th>\n",
       "      <th>post2</th>\n",
       "      <th>post3</th>\n",
       "      <th>post4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word:</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ice</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snow</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tahoe</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goal</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puck</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       post1  post2  post3  post4\n",
       "word:                            \n",
       "ice        4      4      6      2\n",
       "snow       6      1      0      5\n",
       "tahoe      3      0      0      5\n",
       "goal       0      6      5      1\n",
       "puck       0      4      5      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "c_names = ['post1', 'post2', 'post3', 'post4']\n",
    "words = ['ice', 'snow', 'tahoe', 'goal', 'puck']\n",
    "post_words = pd.DataFrame([[4, 4, 6, 2],\n",
    "                           [6, 1, 0, 5],\n",
    "                           [3, 0, 0, 5],\n",
    "                           [0, 6, 5, 1],\n",
    "                           [0, 4, 5, 0]],\n",
    "                          index = words,\n",
    "                          columns = c_names)\n",
    "post_words.index.names = ['word:']\n",
    "post_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like posts 1 and 4 pertain to skiing, and while posts 2 and 3 are about hockey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine the DataFrame <code>post_words</code> as the matrix $A$, where the entries represent the number of times a given word appears in the post. The singular value decomposition of $A$ can be calculated using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U = \n",
      "-------------------------------------------------\n",
      "      0     1     2     3     4\n",
      "0 -0.63  0.02  0.63  0.23 -0.40\n",
      "1 -0.35 -0.68  0.08 -0.56  0.31\n",
      "2 -0.22 -0.52 -0.50  0.65 -0.09\n",
      "3 -0.52  0.37 -0.59 -0.38 -0.32\n",
      "4 -0.41  0.36 -0.00  0.26  0.80 \n",
      "----------------------------------------\n",
      "None\n",
      "sigma_ = \n",
      "-------------------------------------------------\n",
      "       0\n",
      "0  13.32\n",
      "1   9.26\n",
      "2   2.42\n",
      "3   1.38 \n",
      "----------------------------------------\n",
      "None\n",
      "V = \n",
      "-------------------------------------------------\n",
      "      0     1     2     3\n",
      "0 -0.40 -0.57 -0.63 -0.35\n",
      "1 -0.60  0.33  0.41 -0.60\n",
      "2  0.60 -0.41  0.32 -0.61\n",
      "3 -0.34 -0.63  0.58  0.39 \n",
      "----------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as lin\n",
    "\n",
    "def as_df(nparray):\n",
    "    print \"-------------------------------------------------\\n\",\\\n",
    "    pd.DataFrame(np.round(nparray,decimals=2)),\\\n",
    "    \"\\n----------------------------------------\"\n",
    "\n",
    "U, sigma, V = np.linalg.svd(post_words)\n",
    "\n",
    "\n",
    "print \"U = \"\n",
    "print as_df(np.round(U, decimals=2))\n",
    "\n",
    "print \"sigma_ = \"\n",
    "print as_df(np.round(sigma, decimals=2))\n",
    "\n",
    "print \"V = \"\n",
    "print as_df(np.round(V, decimals=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma \n",
      "-------------------------------------------------\n",
      "       0\n",
      "0  13.32\n",
      "1   9.26\n",
      "2   2.42\n",
      "3   1.38 \n",
      "----------------------------------------\n",
      "None\n",
      "V\n",
      "-------------------------------------------------\n",
      "      0     1     2     3\n",
      "0 -0.40 -0.57 -0.63 -0.35\n",
      "1 -0.60  0.33  0.41 -0.60\n",
      "2  0.60 -0.41  0.32 -0.61\n",
      "3 -0.34 -0.63  0.58  0.39 \n",
      "----------------------------------------\n",
      "None\n",
      "sigma * V\n",
      "-------------------------------------------------\n",
      "       0\n",
      "0  -9.84\n",
      "1  -6.39\n",
      "2  -3.04\n",
      "3 -11.14 \n",
      "----------------------------------------\n",
      "None\n",
      "sigma * V.T\n",
      "-------------------------------------------------\n",
      "       0\n",
      "0 -12.56\n",
      "1  -4.76\n",
      "2   4.24\n",
      "3  -8.52 \n",
      "----------------------------------------\n",
      "None\n",
      "diag of sigma*V \n",
      "-------------------------------------------------\n",
      "      0     1     2      3\n",
      "0 -9.84  0.00  0.00   0.00\n",
      "1  0.00 -6.39  0.00   0.00\n",
      "2  0.00  0.00 -3.04   0.00\n",
      "3  0.00  0.00  0.00 -11.14 \n",
      "----------------------------------------\n",
      "None\n",
      "U*diag(S*V)\n",
      "-------------------------------------------------\n",
      "      0     1     2     3\n",
      "0  6.16 -0.12 -1.90 -2.62\n",
      "1  3.46  4.33 -0.24  6.24\n",
      "2  2.16  3.32  1.53 -7.23\n",
      "3  5.11 -2.36  1.79  4.23\n",
      "4  4.01 -2.32  0.01 -2.87 \n",
      "----------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "w =np.dot(sigma,V)\n",
    "\n",
    "print \"Sigma \\n\" ,as_df(sigma)\n",
    "\n",
    "print \"V\\n\", as_df(V)\n",
    "\n",
    "\n",
    "\n",
    "print \"sigma * V\\n\", as_df(w)\n",
    "\n",
    "print \"sigma * V.T\\n\", as_df(np.dot(sigma,V.T))\n",
    "\n",
    "print \"diag of sigma*V \\n\", as_df(np.diag(w))\n",
    "\n",
    "print \"U*diag(S*V)\\n\",as_df(np.dot(U[:,:4],np.diag(w)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $\\vec{a}_i = U * \\Sigma * \\vec{v}_i$, that is each column $\\vec{v}_i$ of $V$ defines the entries in that column, $\\vec{a}_i$, of our data matrix, $A$. Let's label V with the identities of the posts using a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post1  post2  post3  post4\n",
      "0  -0.40  -0.57  -0.63  -0.35\n",
      "1  -0.60   0.33   0.41  -0.60\n",
      "2   0.60  -0.41   0.32  -0.61\n",
      "3  -0.34  -0.63   0.58   0.39\n",
      "          0     1     2     3\n",
      "post1 -0.40 -0.57 -0.63 -0.35\n",
      "post2 -0.60  0.33  0.41 -0.60\n",
      "post3  0.60 -0.41  0.32 -0.61\n",
      "post4 -0.34 -0.63  0.58  0.39\n"
     ]
    }
   ],
   "source": [
    "V_df = pd.DataFrame(np.round(V, decimals=2), columns=c_names)\n",
    "print V_df\n",
    "\n",
    "V_T_df = pd.DataFrame(np.round(V, decimals=2),\n",
    " index=c_names)\n",
    "print V_T_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how post1 and post4 agree closely in value in the first two rows of $V$, as do post2 and post3. This indicates that posts 1 and 4 contain similar words (in this case words relating to skiing). However, the agreement is less close in the last two rows, even among related posts. This is because the weights of the last two rows, $\\sigma_3$ and $\\sigma_4$, are small compared to $\\sigma_1$ and $\\sigma_2$. Let's look at the values for the $\\sigma$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "       0\n",
      "0  13.32\n",
      "1   9.26\n",
      "2   2.42\n",
      "3   1.38 \n",
      "----------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sigma\n",
    "print as_df(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma_1$ and $\\sigma_2$ are about an order of magnitude greater than $\\sigma_3$ and $\\sigma_4$, indicating that the values in the first two rows of $V$ are much more important than the values in the last two. In fact we could closely reproduce $A$ using just the first two rows of $V$ and first two columns of $U$, with an error of at most 1 word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_approx = np.matrix(U[:, :2]) * np.diag(sigma[:2]) * np.matrix(V[:2, :])\n",
    "A_approx_dot = np.dot(np.matrix(U[:, :2]),np.dot(np.diag(sigma[:2]),np.matrix(V[:2, :])))\n",
    "\n",
    "A_approx_T = np.dot(U[:,:4],np.dot(np.diag(sigma[:4]),V[:4,:]))\n",
    "\n",
    "\n",
    "print \"A calculated using only the first two components:\\n\"\n",
    "print pd.DataFrame(np.round(A_approx, decimals=2)\n",
    ", index=words, columns=c_names)\n",
    "\n",
    "print \"A_dot calculated using only the first two components:\\n\"\n",
    "print pd.DataFrame(np.round(A_approx_T, decimals=2), index=words, columns=c_names)\n",
    "\n",
    "print \"\\nError from actual value:\\n\"\n",
    "print post_words - A_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help visualize the similarity between posts, $V$ can be displayed as an image. Notice how the similar posts (1 and 4, 2 and 3) have similar color values in the first two rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(V, interpolation='none')\n",
    "plt.xticks(xrange(len(c_names)))\n",
    "plt.yticks(xrange(len(words)))\n",
    "plt.ylim([len(words) - 1.5, -.5])\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(c_names)\n",
    "ax.set_yticklabels(xrange(1, len(words) + 1))\n",
    "plt.title(\"$V$\")\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing the singular value decomposition tells us is what most defines the different categories of posts. The skiing posts have very different values from the hockey posts in the second row of $V$, i.e. $V_{2,1} \\approx V_{2, 4}$ and $V_{2,2} \\approx V_{2, 3}$ but $V_{2,1} \\neq V_{2, 2}$.\n",
    "\n",
    "Recall from above that:\n",
    "\n",
    "$\\vec{a}_i = \\vec{u}_1 * \\sigma_1 * V_{1,i} + \n",
    "             \\vec{u}_2 * \\sigma_2 * V_{2,i} + ...$\n",
    "             \n",
    "Thus the posts differ very much in how much the values in $\\vec{u}_2$ contribute to their final word count. Here is $\\vec{u}_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(U[:,1], index=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can conclude that, at least in this small data set, the words 'snow' and 'tahoe' identify a different class of posts from the words 'goal' and 'puck'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying similar research papers using singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on from the simple example above, here is an application using singular value decomposition to find similar research papers.\n",
    "\n",
    "I've collect several different papers for analysis. Unfortunately due to the sorry state of open access for scientific papers I can't share the full article text that was used for analysis. <em>Cell</em>, for example, cautions that <b>\"you may not copy, display, distribute, modify, publish, reproduce, store, transmit, post, ...\"</b> Yikes. However I did chose articles such that you should be able to download the pdf's from the publisher for free.\n",
    "\n",
    "<h3>Here are the papers included in analysis (with shortened names in parentheses):</h3>\n",
    "\n",
    "<h4>Two papers on the molecular motor ClpX, describing very similar experiments:</h4>\n",
    "<li><a href=\"http://www.cell.com/retrieve/pii/S0092867411004296\">ClpX(P) Generates Mechanical Force to Unfold and Translocate Its Protein Substrates</a> (clpx1)\n",
    "<li><a href=\"http://www.cell.com/retrieve/pii/S0092867411003138\">Single-Molecule Protein Unfolding and Translocation by an ATP-Fueled Proteolytic Machine</a> (clpx2)\n",
    "\n",
    "<h4>Papers on a very different molecular motor, <a href=\"http://www.frankcleary.com/research\">dynein</a>:</h4>\n",
    "<li><a href=\"http://www.cell.com/fulltext/S0092-8674(12)00928-2\">Lis1 Acts as a “Clutch” between the ATPase and Microtubule-Binding Domains of the Dynein Motor</a> (dyn-lis1)\n",
    "<li><a href=\"http://www.cell.com/abstract/S0092-8674(06)00862-2\">Single-Molecule Analysis of Dynein Processivity and Stepping Behavior</a> (dyn-steps1)\n",
    "<li><a href=\"https://reck-peterson.med.harvard.edu/sites/reck-peterson.med.harvard.edu/files/publication_pdf/Qiu_2012.pdf\">Dynein achieves processive motion using both stochastic and coordinated stepping</a> (dyn-steps2)\n",
    "<li><a href=\"http://www2.mrc-lmb.cam.ac.uk/groups/cartera/pdffiles/2012_Schmidt_NSMB.pdf\">Insights into dynein motor domain function from a 3.3-A crystal structure</a> (dyn-structure)\n",
    "\n",
    "<h4>A paper on T-cell signaling:</h4>\n",
    "<li><a href=\"https://valelab.ucsf.edu/external/publications/2012jamesNature.pdf\">Biophysical mechanism of T-cell receptor triggering in a reconsistuted system</a> (tcell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll need to read in the word counts for each paper. I used python <a href=\"http://www.unixuser.org/~euske/python/pdfminer/\">PDFMiner</a> to convert the pdf documents to plain text. I also used a list of \"stop words\" (<a href=\"http://norm.al/2009/04/14/list-of-english-stop-words/\">link</a>), words such as \"the\", and \"and\", that appear in all English documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('input/stopwords.txt') as f:\n",
    "    stopwords = f.read().strip().split(',')\n",
    "    stopwords = set(stopwords)  # use a set for fast membership testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import re\n",
    "\n",
    "def word_count(fname):\n",
    "    \"\"\"Return a collections.Counter instance counting\n",
    "    the words in file fname.\"\"\"\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        file_content = f.read()\n",
    "        words = re.split(r'\\W+', file_content.lower())\n",
    "        words = [word for word in words \n",
    "                 if len(word) > 3 and word not in stopwords]\n",
    "        word_count = collections.Counter(words)\n",
    "        return word_count\n",
    "        \n",
    "        \n",
    "file_list = ['input/papers/' + f for f in os.listdir('input/papers/')\n",
    "             if f.endswith('.txt')]\n",
    "word_df = pd.DataFrame()\n",
    "for fname in file_list:\n",
    "    word_counter = word_count(fname)\n",
    "    file_df = pd.DataFrame.from_dict(word_counter,\n",
    "                                     orient='index')\n",
    "    file_df.columns = [fname.replace('input/papers/', '').replace('.txt', '')]\n",
    "    # normalize word count by the total number of words in the file:\n",
    "    file_df.ix[:, 0] = file_df.values.flatten() / float(file_df.values.sum())\n",
    "    word_df = word_df.join(file_df, how='outer', )\n",
    "\n",
    "word_df = word_df.fillna(0)\n",
    "print \"Number of unique words: %s\" % len(word_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results, sorted by the most common words in the first paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_df.sort(columns=word_df.columns[0], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to calculate the singular value decomposition of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U, sigma, V = np.linalg.svd(word_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a look at $V$, with the column names added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v_df = pd.DataFrame(V, columns=word_df.columns)\n",
    "v_df.apply(lambda x: np.round(x, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the values of $V$ represented as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(V, interpolation='none')\n",
    "ax = plt.gca()\n",
    "plt.xticks(xrange(len(v_df.columns.values)))\n",
    "plt.yticks(xrange(len(v_df.index.values)))\n",
    "plt.title(\"$V$\")\n",
    "ax.set_xticklabels(v_df.columns.values, rotation=90)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how in the above image, in the first three rows the similarities between the clpx papers is apparent, as well as between the first three dyn papers. The last dyn paper is somewhat different, but this is to be expected since it is a structure paper and the other three dyn papers involve more microscopy. In terms of comparing the papers, singular value decomposition allowed us to reduce the 5657 different words found in the papers into 6 values that are pre-sorted in order of importance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll look in more detail at how similar each paper is to the others. I've defined a function to calculate the distance between two column vectors of $V$, weighted by the weights in $\\Sigma$. For $\\vec{v}_i$ and $\\vec{v}_j$ the function calculates $\\|\\Sigma * (\\vec{v}_i - \\vec{v}_j)\\|$. This function is applied to every pairwise combination of $\\vec{v}_i$ and $\\vec{v}_j$, giving a metric of how similar two papers are (smaller values are more similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dist(col1, col2, sigma=sigma):\n",
    "    \"\"\"Return the norm of (col1 - col2), where the differences in \n",
    "    each dimension are wighted by the values in sigma.\"\"\"\n",
    "    return np.linalg.norm(np.array(col1 - col2) * sigma)\n",
    "\n",
    "dist_df = pd.DataFrame(index=v_df.columns, columns=v_df.columns)\n",
    "for cname in v_df.columns:\n",
    "    dist_df[cname] = v_df.apply(lambda x: dist(v_df[cname].values, x.values))\n",
    "plt.imshow(dist_df.values, interpolation='none')\n",
    "ax = plt.gca()\n",
    "plt.xticks(xrange(len(dist_df.columns.values)))\n",
    "plt.yticks(xrange(len(dist_df.index.values)))\n",
    "ax.set_xticklabels(dist_df.columns.values, rotation=90)\n",
    "ax.set_yticklabels(dist_df.index.values)\n",
    "plt.title(\"Similarity between papers\\nLower value = more similar\")\n",
    "plt.colorbar()\n",
    "dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two clpx papers and the two dyn-steps are most similar to each other, as expected, while all the dyn paper do bear some similarity to each other. For a quicker readout, I've grouped the data into three similarity levels (in practice this could be done automatically with a clustering algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "levels = [0.06, 0.075]\n",
    "binned_df = dist_df.copy()\n",
    "binned_df[(dist_df <= levels[0]) & (dist_df > 0)] = 1\n",
    "binned_df[(dist_df <= levels[1]) & (dist_df > levels[0])] = 2\n",
    "binned_df[(dist_df < 1) & (dist_df > levels[1])] = 3\n",
    "plt.imshow(binned_df.values, interpolation='none')\n",
    "ax = plt.gca()\n",
    "plt.xticks(xrange(len(binned_df.columns.values)))\n",
    "plt.yticks(xrange(len(binned_df.index.values)))\n",
    "ax.set_xticklabels(binned_df.columns.values, rotation=90)\n",
    "ax.set_yticklabels(binned_df.index.values)\n",
    "plt.title(\"Similarity between papers\\nLower value = more similar\")\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's output a list for each paper of the other papers, sorted in order of decreasing similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for paper in dist_df.columns:\n",
    "    sim_papers_df = dist_df.sort(columns=paper)[paper]\n",
    "    sim_papers = sim_papers_df.drop([paper]).index\n",
    "    print 'Papers most similar to ' + paper + ':'\n",
    "    print ', '.join(sim_papers)\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See also: <a href=\"http://www.frankcleary.com/svdimage\">SVD Image Compression</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
