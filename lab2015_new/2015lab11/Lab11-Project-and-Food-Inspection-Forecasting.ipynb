{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-109: Fall 2015  -- Lab 11\n",
    "\n",
    "***\n",
    "In today's lab we'll talk about:\n",
    "1. Project Guidelines\n",
    "2. Go through an example project with lots of key elements.\n",
    "3. In the middle of (2.) we'll see Elastic Net regularization as an extension of Lasso. What's a lab without learning?!\n",
    "\n",
    "This is the main file for the lab, `Lab11-2-Data-Processing.ipynb` is supplemental. \n",
    "***\n",
    "\n",
    "## But first, this:\n",
    "You'll be working in groups now, we encourage you to continue to use `git` but **constant communication** is key. [Slack](https://slack.com/) may be a good option for this!\n",
    "\n",
    "[<img src=\"./images/git.png\">](https://xkcd.com/1597/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Guidelines:\n",
    "Much of this is taken directly from the [Project Page](http://cs109.github.io/2015/pages/projects.html) on the CS109 website.\n",
    "\n",
    "> Goal: to go through the complete data science process to answer questions you have about some topic.\n",
    ">\n",
    "> How?\n",
    "> - acquire the data\n",
    "> - design your visualizations\n",
    "> - run statistical analysis\n",
    "> - communicate the results\n",
    "\n",
    "### Project Milestones:\n",
    "- **Proposal**: Due 11/12/2015\n",
    "- **Proposal Review**: You'll be assigned a TF as an advisor, they'll review your proposal and meet with your **entire** team.\n",
    "\n",
    "\n",
    "### Data science as a process\n",
    "You'll start with an initial idea, your TF should be able to guide you as to the viability of such a proposal. However, as with any project, you'll run into roadblocks! It's important for us to see the entire process and not just the final product. **Your proposal is a great jumping-off point for some of this!**\n",
    "\n",
    ">#### IPython Process Book\n",
    "\n",
    ">An important part of your project is your iPython process book. Your process book details your steps in developing your solution, including how you collected the data, alternative solutions you tried, describing statistical methods you used, and the insights you got. Equally important to your final results is how you got there! Your process book is the place you describe and document the space of possibilities you explored at each step of your project. We strongly advise you to include many visualizations in your process book.Your process book should include the following topics. Depending on your project type the amount of discussion you devote to each of them will vary:\n",
    "\n",
    "> - **Overview and Motivation**: Provide an overview of the project goals and the motivation for it. Consider that this will be read by people who did not see your project proposal.\n",
    "\n",
    "> - **Related Work**: Anything that inspired you, such as a paper, a web site, or something we discussed in class.\n",
    "\n",
    "> - **Initial Questions**: What questions are you trying to answer? How did these questions evolve over the course of the project? What new questions did you consider in the course of your analysis? \n",
    "\n",
    "> - **Data**: Source, scraping method, cleanup, storage, etc.\n",
    "\n",
    "> - **Exploratory Data Analysis**: What visualizations did you use to look at your data in different ways? What are the different statistical methods you considered? Justify the decisions you made, and show any major changes to your ideas. How did you reach these conclusions?\n",
    "\n",
    "> - **Final Analysis**: What did you learn about the data? How did you answer the questions? How can you justify your answers?\n",
    "\n",
    "> - **Presentation**: Present your final results in a compelling and engaging way using text, visualizations, images, and videos on your project web site.\n",
    "\n",
    ">Describe the storytelling elements and goals in your process notebook and show us sketches and screenshots of different web site iterations. As this will be your only chance to describe your project in detail make sure that your process book is a standalone document that fully describes your process and results.\n",
    "\n",
    "### Final Products\n",
    "- **Code**: you've been graded all semester on this, just keep doing a great job at submitting clean code with comments\n",
    "- **Website**: make a *public website* (Github pages, Google sites, etc.), summarize the main results of your project and tell a story, **anyone** should be able to read this part of your project, link to your Process Notebook and Github repo. \n",
    "- **Two-minute presentation**: Each team will create a **two minute** screencas (seriously, 2 minutes) with narration showing a demo of your iPython process book and/or some slides.  Upload to YouTube (or Vimeo) and **embed it into your project web page**. Focus on:\n",
    "    - main contributions rather than on technical details. \n",
    "    - What do you feel is the best part of your project? \n",
    "    - What insights did you gain? \n",
    "    - What is the single most important thing you would like your audience to take away? \n",
    "    - This should be **one of the first things** someone visiting your website should see.\n",
    "\n",
    "### Peer Assessment\n",
    "Remember, data science is an interdisciplinary field, you'll never work alone. It's important that everyone contributes and works well together. So we added this portion to get everyone thinking about teamwork early on.\n",
    "\n",
    "> Peer assessment:\n",
    "> - **Preparation**: were they prepared during team meetings?\n",
    "> - **Contribution**: did they contribute productively to the team discussion and work?\n",
    "> - **Respect for othersâ€™ ideas**: did they encourage others to contribute their ideas?\n",
    "> - **Flexibility**: were they flexible when disagreements occurred?\n",
    "\n",
    "### Grading Criteria\n",
    "\n",
    "> - **Project Scope** - Did you choose the appropriate complexity and level of difficulty of your project?\n",
    "> - **Process Book** - Did you follow the data science process and is it well documented in your process book?\n",
    "> - **Solution** - Is your analysis effective and correct in answering your intended questions?\n",
    "> - **Implementation** - What is the quality of your code? Is it appropriately polished, robust, and reliable?\n",
    "> - **Presentation** - Are your web site and screencast clear, engaging, and effective?\n",
    "> - **Peer Evaluations** - Your individual project score will also be influenced by your peer evaluations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Example: Food Inspection Forecasting\n",
    "\n",
    "## **HUGE NOTE:  Much of this is taken from the [food-inspections-evaluation](https://github.com/Chicago/food-inspections-evaluation) repository. It's publicly available, so if you're interested please check it out further.**\n",
    "\n",
    "Some articles talking about the project:\n",
    " - [Washington Post, September 28, 2014](https://www.washingtonpost.com/business/on-it/in-chicago-food-inspectors-are-guided-by-big-data/2014/09/27/96be8c68-44e0-11e4-b47c-f5889e061e5f_story.html)\n",
    " - [Chicago Tribune, July 6, 2015](http://www.chicagotribune.com/business/ct-chicago-allstate-inspections-0703-biz-20150706-story.html)\n",
    "\n",
    "Let's check out the website they built for their project (built with Gihub pages):\n",
    "\n",
    "[<img src=\"./images/chicago_page.png\">](http://chicago.github.io/food-inspections-evaluation/)\n",
    "\n",
    "We're looking for some of the major elements we discussed above. \n",
    "\n",
    "Some questions about the webpage:\n",
    "\n",
    "- Is the writing directed appropriately?\n",
    "- Motivation?\n",
    "- Do they link to their sources?\n",
    "\n",
    "Goal: \n",
    "> To forecast food establishments that are most likely to have critical violations so that they may be inspected first.\n",
    "\n",
    "Main Result:\n",
    "> <img src=\"./images/main_result.png\">\n",
    "\n",
    "Data: \n",
    "> https://data.cityofchicago.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Data:\n",
    "\n",
    "The entire analysis was done in R, I used their code to processed the data to the point where I could use it easily. See `Data-Processing.ipynb`. This is an ipython notebook that uses `R-magic`. If you don't know `R` you can just download the files with processed data into your lab directory.\n",
    "\n",
    "**OR, just follow along without running the code**. Running the code isn't as important for this lab.\n",
    "\n",
    "Download the following files (right-click -> \"save file as\"): (1-2) are important for running the model below, (1) is just for data exploration.\n",
    "1. [Model Matrix](https://www.dropbox.com/s/t90lh34kuyt2byf/model_matrix.csv?dl=0)\n",
    "2. [Outcome](https://www.dropbox.com/s/nj8jtuqisk74qj6/TARGET.csv)\n",
    "3. [Food Inspections data (huge)](https://www.dropbox.com/s/6x1dcin3viwyumy/food_inspections.csv?dl=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# special IPython command to prepare the notebook for matplotlib\n",
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "# special matplotlib argument for improved plots\n",
    "from matplotlib import rcParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File food_inspections.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-62e684a32b0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'food_inspections.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mfood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramon/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramon/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramon/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramon/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramon/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File food_inspections.csv does not exist"
     ]
    }
   ],
   "source": [
    "food = pd.read_csv('food_inspections.csv', sep=',')\n",
    "\n",
    "print food.shape\n",
    "food.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focus in on 2014 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import dateutil.parser\n",
    "food['year'] = [dateutil.parser.parse(x).year for x in food.Inspection_Date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "food2014 = food.loc[food['year'].values == 2014,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print food2014.shape\n",
    "food2014.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A breakdown of the inspection types in 2014:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "food2014.Inspection_Type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A summary of the risk types appears below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "food2014.Risk.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "### Goal of the model\n",
    "> The principle question is whether we can reasonably determine the probability that a restaurant inspection will yield at least one critical violation.\n",
    "\n",
    "Clearly this goal was refined with some experience handling this data, you'll have the same experience as you work with your data. \n",
    "\n",
    "\n",
    "\n",
    "Variable Name (Literal)                       | Variable Description\n",
    "----------------------------------------------|---------------------------------\n",
    "`Inspectorblue`                               | Indicator variable for Sanitarian Cluster 1\n",
    "`Inspectorbrown`                              | Indicator variable for Sanitarian Cluster 2\n",
    "`Inspectorgreen`                              | Indicator variable for Sanitarian Cluster 3\n",
    "`Inspectororange`                             | Indicator variable for Sanitarian Cluster 4\n",
    "`Inspectorpurple`                             | Indicator variable for Sanitarian Cluster 5\n",
    "`Inspectoryellow`                             | Indicator variable for Sanitarian Cluster 6\n",
    "`pastCritical`                                | Indicates any previous critical violations (last visit)\n",
    "`pastSerious`                                 | Indicates any previous serious violations (last visit)\n",
    "`timeSinceLast`                               | Elapsed time since previous inspection\n",
    "`ageAtInspection`                             | Age of business license at the time of inspection\n",
    "`consumption_on_premises_incidental_activity` | Presence of a license for consumption / incidental activity\n",
    "`tobacco_retail_over_counter`                 | Presence of an additional license for tobacco sales\n",
    "`temperatureMax`                              | The daily high temperature on the day of inspection\n",
    "`heat_burglary`                               | Local intensity of recent burglaries\n",
    "`heat_sanitation`                             | Local intensity of recent sanitation complaints\n",
    "`heat_garbage`                                | Local intensity of recent garbage cart requests\n",
    "\n",
    "\n",
    "\n",
    "## Elastic Net Regularized Lostic Regression\n",
    "\n",
    "#### Recall: Logistic Regression with Lasso\n",
    "We've seen Logistic regression with the Lasso penalty, a form of regularization and variable selection. As a reminder, the Logistic regression with Lasso minimizes the equation:\n",
    "\n",
    "$$\n",
    "\\min_{(\\beta_0, \\beta) \\in \\mathbb{R}^{p+1}} -\\left[\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot (\\beta_0 + x_i^T \\beta) - \\log (1+e^{(\\beta_0+x_i^T \\beta)})\\right] + \\alpha\\ ||\\beta||_1\n",
    "$$\n",
    "\n",
    "We're essentially looking for solutions $(\\beta_0, \\beta)$ that are **small** in terms of the $L_1$ norm. We've also seen Ridge regression, another type of regularization, essentially the same but with $\\lambda\\ ||\\beta||_2^2$ instead of the $L_1$ norm. \n",
    "\n",
    "\n",
    "####  Elastic Net Regularized Logistic Regression\n",
    "For the final model they decided to use Elastic Net regularization on a logistic regression. This is essentially a combination of Lasso and Ridge regularization, you can see it below in the formulation. \n",
    "\n",
    ">$$\n",
    "\\begin{aligned}\n",
    "\\log = \\frac{\\text{Pr}(V=1|X=x)}{\\text{Pr}(V=0|X=x)} = \\beta_0 + \\beta^T x\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    ">Thus, the objective function is to minimize \n",
    ">\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{(\\beta_0, \\beta) \\in \\mathbb{R}^{p+1}} -\\left[\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot (\\beta_0 + x_i^T \\beta) - \\log (1+e^{(\\beta_0+x_i^T \\beta)})\\right] + \\alpha (1-\\lambda)||\\beta||_2^2/2 + \\alpha\\lambda||\\beta||_1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In our code $\\lambda =$ `l1_ratio`, and an $\\alpha$ range is determined by `eps`, see the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) for more details. \n",
    "\n",
    "**Note: They didn't justify their use of this model. We know that prediction is their end-goal. I would like to know what other methods they tried or if they have a specific reason for using this model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mm = pd.read_csv('model_matrix.csv', sep=',')\n",
    "X = mm.values.astype(np.double)\n",
    "y = pd.read_csv('TARGET.csv', sep=',').values.astype(np.double).transpose()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lasso_path, enet_path\n",
    "eps = 5e-6  # the smaller it is the longer is the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Computing regularization path using the lasso...\")\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept=False)\n",
    "\n",
    "print(\"Computing regularization path using the elastic net...\")\n",
    "alphas_enet, coefs_enet, _ = enet_path(X, y, eps=eps, l1_ratio=0.8, fit_intercept=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Color Palette, seaborn has the best colors i.m.o.\n",
    "col = sns.color_palette(\"husl\", 16)\n",
    "\n",
    "# make the lasso path\n",
    "plt.figure(1)\n",
    "ax = plt.gca()\n",
    "ax.set_color_cycle(col)\n",
    "l1 = plt.plot(-np.log10(alphas_lasso), coefs_lasso.T)\n",
    "l2 = plt.plot(-np.log10(alphas_enet), coefs_enet.T, linestyle='--')\n",
    "plt.xlim([1, 4])\n",
    "\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Lasso and Elastic-Net Paths')\n",
    "plt.legend((l1[-1], l2[-1]), ('Lasso', 'Elastic-Net'), loc='lower left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset variables with large coefficients\n",
    "# selected somewhat arbitrarily, could do something better here.\n",
    "\n",
    "# show all coefficients\n",
    "# keep = np.ones(16, dtype=bool)\n",
    "# show only if sum of coefficients larger than something\n",
    "# keep = np.abs(coefs_enet.T.sum(axis = 0)) > 0.5\n",
    "# show only if least shrunk coefficient larger than something\n",
    "keep = np.abs(coefs_enet.T[99,:]) > 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Color Palette, seaborn has the best colors i.m.o.\n",
    "col = sns.color_palette(\"hls\", keep.sum())\n",
    "\n",
    "plt.figure(1)\n",
    "ax = plt.gca()\n",
    "ax.set_color_cycle(col)\n",
    "l2 = plt.plot(-np.log10(alphas_enet), coefs_enet.T[:,keep])\n",
    "plt.xlim([1, 4])\n",
    "\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Elastic-Net Paths (largest coefficients)')\n",
    "plt.legend(l2, list(mm.columns.values[keep]), loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Path Charts\n",
    "Both Lasso and Elastic Net regularization have the property that with enough regularization (large $\\alpha$) (or small $-log(\\alpha)$ on the chart) the coefficients ($\\hat\\beta_0, \\hat\\beta_1, ..., \\hat\\beta_p$) get shrunk to 0. That means they're no longer playing a role in the prediction! Recall:\n",
    "\n",
    "$$\\hat y_i = \\frac{exp(\\hat\\beta_0 + \\hat\\beta_1 X_{i1} + ... + \\hat\\beta_p X_{ip})}{1 + exp(\\hat\\beta_0 + \\hat\\beta_1 X_{i1} + ... + \\hat\\beta_p X_{ip})}$$\n",
    "\n",
    "First let's notice that the `Inspector*` variables are all present, except for one -- `InspectorYellow` is missing, but it's easy to see that it wouldn't be informative at all. Simply knowing that the region wasn't any of (`blue`, `brown`, ..., `purple`) will tell you that the inspection was in `yellow`. So the regularization does a good job at getting rid of this redundant information. \n",
    "\n",
    "What this means for us is that we can look at a chart like the one above and see that (from left to right) the first variable to enter the model is `InspectorBlue`, `InspectorPurple`, etc. \n",
    "\n",
    "Other main findings they gathered from this chart:\n",
    "\n",
    "> - Establishments that had previous critical or serious violations\n",
    "> - Whether the establishment has a tobacco license or has an incidental alcohol consumption license\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation/testing\n",
    "The super impressive thing about this analysis (to me) is their way of testing their results. \n",
    "\n",
    "The discovery of a critical violation is a time-sensitive issue! The earlier an inspector can find these critical violations the better. So instead of just looking at things like accuracy, f1 score, etc. they looked for a way of vallidating the model within the contect of the problem. Here's how:\n",
    "\n",
    "- They used the data before **July 31, 2014** to fit the model. \n",
    "- They selected a model parameters using **August 1-31, 2014** as a tuning dataset.\n",
    "- They used the model to predict critical violations in the 1,637 food establishments that were scheduled for **September 1, 2014 to October 31, 2014**. \n",
    "- Based on the predictions they could create a schedule of inspections that would hopefully yield a higher number of critical violations earlier.\n",
    "- They then let the inspectors go on about their business, reposting critical violations as they do.\n",
    "- At the end of the month, they compared what happened to what `could have happened` if they used the schedule based on predictions.\n",
    "\n",
    "> After formulating the analytical model, the the principal question for researchers turned to whether this analytical model provides more efficiency for the food inspection team. CDPH operational procedures requires the department to inspect every risk 1 and risk 2 restaurant. Therefore, the operational goal is to allow inspectors to discover critical violations earlier than their current operations (business-as-usual)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "## Taken from [Report](https://github.com/Chicago/food-inspections-evaluation/blob/master/REPORTS/forecasting-restaurants-with-critical-violations-in-Chicago.pdf)\n",
    "<img src=\"./images/days.png\">\n",
    "<img src=\"./images/cumulative1.png\">\n",
    "<img src=\"./images/efficiency.png\">\n",
    "<img src=\"./images/cumulative2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Some further notes:\n",
    "\n",
    "- Further inspection of their code revealed a very detailed analysis, the selected their tuning parameters using a validation set. \n",
    "- Due to the ongoing nature of the problem, it makes sense to validate on only the latest data.\n",
    "- Further exploration in terms of models could show some improvement.\n",
    "- The writeup contained lots more analysis and information than the website for those who are interested.\n",
    "\n",
    "# MORE PROJECTS:\n",
    "You'll see some previous projects that we thought were also exceptional:\n",
    "http://cs109.github.io/2014/pages/projects.html\n",
    "\n",
    "- Ale Augur: Quantifying and Predicting Beer Preference with the Untapped API by Ian Nightingale, Alexander Jaffe and Brian Mendel ([Website](http://beer.iandnightingale.com/), [Screencast](https://www.youtube.com/watch?v=J6Svwwetaj8))\n",
    "- ClimbRec by Amy Skerry ([Website](http://mindhive.mit.edu/saxe/cs109proj/), [Screencast](https://www.youtube.com/watch?v=41BG2z9SbMg&feature=youtu.be))\n",
    "- The Green Canvas by Ahmed Hosny, Jili Huang and Yingyi Wang ([Website](http://ahmedhosny.github.io/theGreenCanvas/), [Screencast](http://vimeo.com/114379373))\n",
    "- Predicting Citation Counts of arXiv Papers by Marion Dierickx, George Miller, Sam Sinai and Stephen Portillo ([Website](https://sites.google.com/site/teamcitations/), [Screencast](http://youtu.be/bjAcHu8JRG0))\n",
    "- Improving University Energy Efficiency: Building Energy Demand Prediction by Wen Xie, Wette Constant and Bin Yan ([Website](http://cs109-energy.github.io/), [Screencast](http://youtu.be/JiAQxctOntQ))\n",
    "- Predicting AirBnb Success by Enrique Dominguez-Meneses, Jameson Rogers, Noah Taylor and Muhammed Yildiz ([Website](http://hamelsmu.github.io/AirbnbScrape/), [Screencast](https://www.youtube.com/watch?v=raGjUj5qArc))\n",
    "\n",
    "Some of the screencasts are down, but the webpages are still up!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
